from sklearn.model_selection import train_test_split
import numpy as np
import keras
from keras.models import Sequential
from keras.layers.core import Dense,Flatten,Dropout
from keras.layers import Activation,Embedding,TimeDistributed,LSTM,GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy
from keras.layers.convolutional import *
from keras.layers.normalization import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import pandas as pd
from keras.preprocessing import text
from keras.preprocessing import sequence
import re
from nltk.corpus import stopwords
import nltk
from nltk.tokenize.treebank import TreebankWordDetokenizer
from nltk.tokenize import word_tokenize
from tqdm import tqdm
nltk.download('stopwords')
nltk.download('punkt')
from keras.applications.vgg16 import VGG16
from PIL import Image
from numba import jit, cuda
import keras.models



img1=np.array([0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img2=np.array([0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img3=np.array([0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img4=np.array([0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img5=np.array([0,0,0,0,0,0,1,1,1,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img6=np.array([0,0,0,0,0,0,1,1,1,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img7=np.array([0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,1,0,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img8=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img9=np.array([0,0,0,0,0,0,1,1,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0])
img10=np.array([0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img11=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,1,0,1,0,0,0,1,1,0,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img12=np.array([0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img13=np.array([0,0,0,0,0,0,0,1,0,1,0,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img14=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,0,0,1,0,1,0,1,0,1,0,1,0,0,1,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img15=np.array([0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img16=np.array([0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img17=np.array([0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,1,0,1,0,1,0,0,1,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img18=np.array([0,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,1,1,1,0,0,1,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img19=np.array([0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img20=np.array([0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img21=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img22=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img23=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img24=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img25=np.array([0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
img26=np.array([0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
img27=np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])





img2=img2.reshape(10,6)
img1=img1.reshape(10,6)
img3=img3.reshape(10,6)
img4=img4.reshape(10,6)
img5=img5.reshape(10,6)
img6=img6.reshape(10,6)
img7=img7.reshape(10,6)
img8=img8.reshape(10,6)
img9=img9.reshape(10,6)
img10=img10.reshape(10,6)
img11=img11.reshape(10,6)
img12=img12.reshape(10,6)
img13=img13.reshape(10,6)
img14=img14.reshape(10,6)
img15=img15.reshape(10,6)
img16=img16.reshape(10,6)
img17=img17.reshape(10,6)
img18=img18.reshape(10,6)
img19=img19.reshape(10,6)
img20=img20.reshape(10,6)
img21=img21.reshape(10,6)
img22=img22.reshape(10,6)
img23=img23.reshape(10,6)
img24=img24.reshape(10,6)
img25=img25.reshape(10,6)
img26=img26.reshape(10,6)
img27=img27.reshape(10,6)



dict1={'A':img1,'B':img2,'C':img3,'D':img4,'E':img5,'F':img6,'G':img7,'H':img8,'I':img9,'J':img10,'K':img11,'L':img12,'M':img13,'N':img14,'O':img15,'P':img16,'Q':img17,'R':img18,'S':img19,'T':img20,'U':img21,'V':img22,'W':img23,'X':img24,'Y':img25,'Z':img26,' ':img27}

from google.colab import drive
drive.mount('/content/drive')

dataset=pd.read_csv('/content/drive/My Drive/Tweets.csv')

dataset=dataset[dataset['airline_sentiment']!='neutral']

dataset=dataset.reset_index()
dataset=dataset[['text','airline_sentiment']]

dataset.airline_sentiment[dataset.airline_sentiment=='positive'] = 1
dataset.airline_sentiment[dataset.airline_sentiment=='negative'] = 0

dataset_labels=dataset.airline_sentiment

dataset=dataset['text']

dataset

def process(data): 
 text_dataframe=pd.DataFrame()
 text_list=[]
 word_list=[]  
 stop_words=set(stopwords.words("english"))
 for i in tqdm(range(0,len(data))):
    text1=re.sub("@[A-Za-z0-9]+"," ",data[i])
    text1=re.sub("#[A-Za-z0-9]+"," ",text1)
    text1=re.sub("http\S+"," ",text1)
    text1=re.sub("[^A-Za-z]+"," ",text1)
    words=word_tokenize(text1)
    for w in words:
     if w not in stop_words:
           word_list.append(w)
    text1=TreebankWordDetokenizer().detokenize(word_list)
    word_list=[]
    text1=text1.upper()
    text_list.append(text1)    

      


 text_dataframe=text_dataframe.append(text_list)

 return text_dataframe

def process2(data): 
 text_dataframe=pd.DataFrame()
 text_list=[]
 word_list=[]  
 len1=0
 stop_words=set(stopwords.words("english"))
 for i in tqdm(range(0,len(data))):
    text1=re.sub("@[A-Za-z]+"," ",data[i])
    text1=re.sub("#[A-Za-z]+"," ",text1)
    text1=re.sub("http\S+"," ",text1)
    text1=re.sub("[^A-Za-z]+"," ",text1)
    words=word_tokenize(text1)
    for w in words:
     if w not in stop_words:
           word_list.append(w)
           len1+=1
           if len1==5:
                break;
    text1=TreebankWordDetokenizer().detokenize(word_list)
    len1=0
    word_list=[]
    text1=text1.upper()
    text_list.append(text1)   
    
 text_dataframe=text_dataframe.append(text_list)

 return text_dataframe

procs_dataset=process(dataset)

procs_dataset

pro=procs_dataset[0]

vocab_size=1200
tokenizer=text.Tokenizer(num_words=vocab_size)

tokenizer.fit_on_texts(pro)

sequences=tokenizer.texts_to_sequences(pro)

words=tokenizer.sequences_to_texts(sequences)

#words[0:10]

#@title Default title text
len1=0
word_list=[]
text_list=[]
for word in words:
   w=word_tokenize(word)
   for letters in w:
     len1+=1
     word_list.append(letters)
     if len1==5:
       break;
   text1=TreebankWordDetokenizer().detokenize(word_list)
   len1=0
   word_list=[]
   text1=text1.upper()
   text_list.append(text1) 
   print(text_list)
   #text_list.append(text1)
procs_dataset=pd.DataFrame(text_list)

procs_dataset

#list_word=[]
#for w in word:
 #  list_word.append(w.upper())

#procs_dataset=pd.DataFrame(list_word)

joined_data=pd.concat([procs_dataset,dataset_labels],axis=1)

joined_data.replace('',np.nan,inplace=True)
joined_data.dropna(axis=0,how='any',inplace=True)

dataset=joined_data[0]

dataset=pd.DataFrame(dataset)

dataset_labels=joined_data.iloc[:,1:4]

dataset_labels

dataset

data_train,data_test,label_train,label_test=train_test_split(dataset,dataset_labels,test_size=0.2)

def convert_to_images_re(dataframe):
 image_dataset=[]
 for j in dataframe[0]:
     list_j=word_tokenize(j)
     for ch in list_j:
         images=[]
         for chs in (ch):
             img=dict1[chs]
             images.append(img)
                
         image_list=np.concatenate(images,axis=1)
         image_dataset.append(image_list)
     if len(list_j)<5:
      for i in range(len(list_j),5):
            image_dataset.append(dict1[' '])
         
 return image_dataset        


df1=convert_to_images_re(data_train)

df2=convert_to_images_re(data_test)

data_train

plt.imshow(df1[5])

import matplotlib.pyplot as plt
plt.imshow(df1[3])
len(df1[1][-1])

max_len=26
list2=[]
for i in range(0,len(df1)):
    print(i)
    im_length=int(len(df1[i][-1])/6)
    for j in range(im_length,max_len):
        list2.append(dict1[' '])
    list2=np.concatenate(list2,axis=1)
    df1[i]=np.concatenate((df1[i],list2),axis=1)
    list2=[]

max_len=26
list3=[]
for i in range(0,len(df2)):
    print(i)
    im_length=int(len(df2[i][-1])/6)
    for j in range(im_length,max_len):
        list3.append(dict1[' '])
    list3=np.concatenate(list3,axis=1)
    df2[i]=np.concatenate((df2[i],list3),axis=1)
    list3=[]

model=Sequential()
model.add(TimeDistributed(Conv2D(32, (3,3), 
            padding='same', strides=(2,2), activation='relu'),
        input_shape = (5, 10, 156, 1)))
model.add(TimeDistributed(Conv2D(64, (3,3), 
            padding='same', strides=(2,2), activation='relu')))
model.add(TimeDistributed(Conv2D(128, (3,3), 
            padding='same', strides=(2,2), activation='relu')))


model.add(
    TimeDistributed(
        GlobalAveragePooling2D() # Or Flatten()
    ))
model.add(LSTM(1024,activation='relu',return_sequences=False))
model.add(Dense(1024,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1,activation='sigmoid'))

model2=Sequential()
model2.add(TimeDistributed(Conv2D(32, (3,3), 
            padding='same', strides=(2,2), activation='relu'),
        input_shape = (5, 10, 156, 1)))
model2.add(TimeDistributed(Conv2D(64, (3,3), 
            padding='same', strides=(2,2), activation='relu')))
model2.add(TimeDistributed(Conv2D(128, (3,3), 
            padding='same', strides=(1,1), activation='relu')))
model2.add(TimeDistributed(MaxPooling2D((2,2),strides=(2,2))))
model2.add(Flatten())
model2.add(Dense(1054,activation='relu'))
model2.add(Dense(1,activation='softmax'))

model.compile('adam', loss='binary_crossentropy',metrics=['accuracy'])

dfs=np.array(df1)

dfs.shape

dfs=dfs.reshape(-1,10,156,1)

dfs=dfs.reshape(-1,5,10,156,1)

dfs.shape

from sklearn.metrics import confusion_matrix
  import itertools

model.fit(dfs,label_train,validation_split=0.2,batch_size=100,epochs=40)

dfs2=np.array(df2)

dfs2.shape

dfs2=dfs2.reshape(-1, 10, 156,1)

dfs2=dfs2.reshape(-1,5,10,156,1)

dfs2.shape

predictions=model.predict(dfs2)

predictions

for i in range(0,len(predictions)):
   if predictions[i]>0.5:
     predictions[i]=1
   else:
     predictions[i]=0

results=model.evaluate(dfs2,label_test)

results

from sklearn.metrics import confusion_matrix
  import itertools

label_test

label_test_array=np.array(label_test)

label_test_array

label_test_array(np.array(label_test))

label_test

label_test_array



for i in predictions:
  print(i)

cm = confusion_matrix(label_test_array,predictions)

def plot_confusion_matrix(cm,classes,normalize=False,title='confusion_matrix',cmap=plt.cm.Blues):
    plt.imshow(cm,interpolation='nearest',cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks=np.arange(len(classes))
    plt.xticks(tick_marks,classes,rotation=45)
    plt.yticks(tick_marks,classes)
    
    if normalize:
        cm=cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]
        print('normalized confusion matrix')
    else:
        print('confusion matrix,without normalization')
    print(cm)
    
    thresh=cm.max()/2.
    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):
        plt.text(j,i,cm[i,j], 
                 horizontalalignment="center",
                 color='white' if cm[i,j] > thresh else 'black')
    plt.tight_layout()
    plt.ylabel('true_label')
    plt.xlabel('predicted_label')

cm_plot_labels=['negative','positive']
plot_confusion_matrix(cm,cm_plot_labels,title='Confusion_matirx')

from sklearn.metrics import classification_report
print(classification_report(label_test_array,predictions))
